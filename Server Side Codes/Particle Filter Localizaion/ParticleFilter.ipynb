{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import kf_book.pf_internal as pf_internal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import uniform\n",
    "from filterpy.stats import plot_gaussian_pdf\n",
    "import csv\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import math as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import uniform\n",
    "\n",
    "def create_uniform_particles(x_range, y_range, hdg_range, N):\n",
    "    particles = np.empty((N, 3))\n",
    "    particles[:, 0] = uniform(x_range[0], x_range[1], size=N)\n",
    "    particles[:, 1] = uniform(y_range[0], y_range[1], size=N)\n",
    "    particles[:, 2] = uniform(hdg_range[0], hdg_range[1], size=N)\n",
    "    particles[:, 2] %= 2 * np.pi\n",
    "    return particles\n",
    "\n",
    "def create_gaussian_particles(mean, std, N):\n",
    "    particles = np.empty((N, 3))\n",
    "    particles[:, 0] = mean[0] + (randn(N) * std[0])\n",
    "    particles[:, 1] = mean[1] + (randn(N) * std[1])\n",
    "    particles[:, 2] = mean[2] + (randn(N) * std[2])\n",
    "    particles[:, 2] %= 2 * np.pi\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "##### Update Step #####\n",
    "#######################\n",
    "\n",
    "def update(particles, weights, z, R):\n",
    "    for particle in particles:\n",
    "        for i, beacon in enumerate(beacons):\n",
    "            distance = \n",
    "            # distance = np.linalg.norm(particles[:, 0:2] - beacon, axis=1)\n",
    "            weights *= scipy.stats.norm(distance, R).pdf(z[i])\n",
    "\n",
    "    weights += 1.e-300      # avoid round-off to zero\n",
    "    weights /= sum(weights) # normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "##### State Estimation Step #####\n",
    "#################################\n",
    "\n",
    "def estimate(particles, weights):\n",
    "    \"\"\"returns mean and variance of the weighted particles\"\"\"\n",
    "\n",
    "    pos = particles[:, 0:2]\n",
    "    mean = np.average(pos, weights=weights, axis=0)\n",
    "    var  = np.average((pos - mean)**2, weights=weights, axis=0)\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "#### Particle Resampling ####\n",
    "#############################\n",
    "\n",
    "def simple_resample(particles, weights):\n",
    "    N = len(particles)\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    cumulative_sum[-1] = 1. # avoid round-off error\n",
    "    indexes = np.searchsorted(cumulative_sum, random(N))\n",
    "\n",
    "    # resample according to indexes\n",
    "    particles[:] = particles[indexes]\n",
    "    weights.fill(1.0 / N)\n",
    "    \n",
    "def resample_from_index(particles, weights, indexes):\n",
    "    particles[:] = particles[indexes]\n",
    "    weights[:] = weights[indexes]\n",
    "    weights.fill(1.0 / len(weights))    \n",
    "\n",
    "def neff(weights):\n",
    "    return 1. / np.sum(np.square(weights))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################\n",
    "#### Transition Matrix Training ####\n",
    "####################################\n",
    "\n",
    "def PredLookup(item, index, predlist):\n",
    "    # print(index)\n",
    "    # print(predlist[index - 1][1])\n",
    "    if (predlist[index - 1][1] == item):\n",
    "        return False;\n",
    "    else:\n",
    "        return True;\n",
    "    \n",
    "\n",
    "agenttrace = '../4jul11p.csv'\n",
    "f = open(agenttrace)\n",
    "#print(f.readlines)\n",
    "oracle = []\n",
    "gridized_data = []\n",
    "gridsize = 0.5\n",
    "\n",
    "xbins = 13\n",
    "ybins = 21\n",
    "\n",
    "# Based on Smart Condo Dimension and orientation:\n",
    "TMatrix = np.zeros((xbins * ybins, xbins * ybins))\n",
    "TLookupTable = []\n",
    "\n",
    "for x in range(xbins):\n",
    "    for y in range(ybins):\n",
    "        TLookupTable.append(\"[\" + str(x) + \", \" + str(y) + \"]\")\n",
    "        \n",
    "with open(agenttrace, 'r') as user_data:\n",
    "    reader = csv.reader(user_data, delimiter=',', quotechar=',')\n",
    "    for row in reader:\n",
    "        if (reader.line_num > 1):\n",
    "            oracle.append([float(row[4]), float(row[5])])\n",
    "            \n",
    "for d in oracle:\n",
    "    gridized_data.append([int(np.floor(d[0]/gridsize)), int(np.floor(d[1]/gridsize))])        \n",
    "    \n",
    "for x in range(xbins):\n",
    "    for y in range(ybins):\n",
    "        # find (x, y)s in gridized_data\n",
    "        indices = [index for index, item in enumerate(gridized_data) if item == [x, y] and \n",
    "                   PredLookup(item, index, list(zip(gridized_data[1:], gridized_data)))]  \n",
    "        \n",
    "        # foreach (x, y): find their next successor; N = total number of successors \n",
    "        if not indices:\n",
    "            print(\"Agent was not in location [\" + str(x) + \", \" + str(y) + \"] at all\")\n",
    "        else:\n",
    "            # print([x, y])\n",
    "            # print(operator.itemgetter(*indices)(gridized_data))\n",
    "            print(indices)\n",
    "            # print(gridized_data[for i in indices])\n",
    "            \n",
    "            print(\"shadan\")\n",
    "            N = len(indices)\n",
    "            next_indices = [x + 1 for x in indices]\n",
    "            \n",
    "            next_states = [gridized_data[i] for i in next_indices]\n",
    "            \n",
    "            # next_states = [operator.itemgetter(*next_indices)(gridized_data)]\n",
    "            \n",
    "            print(next_states)\n",
    "            \n",
    "            print(\"golestan\")\n",
    "            \n",
    "            for next_state in next_states:\n",
    "                n = next_states.count(next_state)\n",
    "                # print(TLookupTable)\n",
    "                # print(TLookupTable.index(next_state))\n",
    "                \n",
    "                cell = TLookupTable.index(str(next_state))\n",
    "                TMatrix[x, cell] = float(n / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix(transitions, xbins, ybins):\n",
    "    # states_lookup_table = []\n",
    "    # for x in range(xbins):\n",
    "        # for y in range(ybins):\n",
    "            # states_lookup_table.append(\"[\" + str(x) + \", \" + str(y) + \"]\")\n",
    "\n",
    "    # n = 1 + len(np.array(list(set(tuple(p) for p in transitions)))) #number of unique states\n",
    "    M = np.ones((xbins * ybins, xbins * ybins))\n",
    "    \n",
    "    for (state, state_p) in zip(transitions,transitions[1:]):\n",
    "        # print(\"state is: \")\n",
    "        # print(state)\n",
    "        # print(\"state prime is: \")\n",
    "        # print(state_p)\n",
    "        # i = state[0] * (ybins - 1) + state[1]\n",
    "        # j = state_p[0] * (ybins - 1) + state_p[1]\n",
    "        \n",
    "        i = xbins * state[1] + state[0] + 1\n",
    "        j = xbins * state_p[1] + state_p[0] + 1\n",
    "        \n",
    "        # print(i)\n",
    "        # print(j)\n",
    "        # i = state[0]*xbins + state[1]\n",
    "        # j = state_p[0]*xbins + state_p[1]\n",
    "        # i = states_lookup_table.index(str(state))\n",
    "        # j = states_lookup_table.index(str(state_p))\n",
    "        M[i][j] += 1\n",
    "\n",
    "    #now convert to probabilities:\n",
    "    for row in M:\n",
    "        s = sum(row)\n",
    "        if s > 0:\n",
    "            row[:] = [f/s for f in row]\n",
    "    return M\n",
    "\n",
    "def beacon_sensor_model():\n",
    "    filename = '../RSSI.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test:\n",
    "\n",
    "agenttrace = '../4jul11p.csv'\n",
    "f = open(agenttrace)\n",
    "#print(f.readlines)\n",
    "oracle = []\n",
    "gridized_data = []\n",
    "\n",
    "# Information Granularity\n",
    "                \n",
    "gridsize = 0.5\n",
    "realx = 6.6\n",
    "realy = 10.6\n",
    "                \n",
    "xbins = int(np.ceil(realx) / gridsize)\n",
    "ybins = int(np.ceil(realy) / gridsize)\n",
    "\n",
    "\n",
    "# print(xbins)\n",
    "# print(ybins)\n",
    "\n",
    "with open(agenttrace, 'r') as user_data:\n",
    "    reader = csv.reader(user_data, delimiter=',', quotechar=',')\n",
    "    for row in reader:\n",
    "        if (reader.line_num > 1):\n",
    "            oracle.append([float(row[4]), float(row[5])])\n",
    "            \n",
    "for d in oracle:\n",
    "    gridized_data.append([int(np.floor(d[0]/gridsize)), int(np.floor(d[1]/gridsize))])        \n",
    "\n",
    "# print(np.max(gridized_data))\n",
    "\n",
    "TM = transition_matrix(gridized_data, xbins, ybins)\n",
    "\n",
    "# print(len(TM[:,1]))\n",
    "# print(len(TM[1,:]))\n",
    "\n",
    "# for row in m: \n",
    "#     print(' '.join('{0:.2f}'.format(x) for x in row))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "gridsize = 0.5\n",
    "realx = 6.6\n",
    "realy = 10.6\n",
    "                \n",
    "xbins = int(np.ceil(realx) / gridsize) + 1\n",
    "ybins = int(np.ceil(realy) / gridsize) + 1\n",
    "\n",
    "print(xbins)\n",
    "print(ybins)\n",
    "\n",
    "states_lookup_table = []\n",
    "for x in range(xbins):\n",
    "        for y in range(ybins):\n",
    "            states_lookup_table.append(\"[\" + str(x) + \", \" + str(y) + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(agenttrace, 'r') as user_data:\n",
    "    reader = csv.reader(user_data, delimiter=',', quotechar=',')\n",
    "    for row in reader:\n",
    "        if (reader.line_num > 1):\n",
    "            oracle.append([float(row[4]), float(row[5])])\n",
    "            \n",
    "for d in oracle:\n",
    "    gridized_data.append([int(np.floor(d[0]/gridsize)), int(np.floor(d[1]/gridsize))])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#### Configurations ####\n",
    "########################\n",
    "\n",
    "beacons = np.array([[1, 1], [3, 4.9], [1.35, 7.7], [3.3, 10.5], [3.3, 10.5], [6, 0.5], [0.5, 3.3], [6, 10.5]])\n",
    "NL = len(beacons)\n",
    "\n",
    "pik = 'SensorModel.dat'\n",
    "\n",
    "with open(pik, 'rb') as f:\n",
    "    beacons_mean = pickle.load(f)\n",
    "    beacons_stds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CumulativeSum(m):\n",
    "    return m.cumsum()\n",
    "\n",
    "def returnobj(cdf):\n",
    "    # print(\"aaaaaaaaaaaabbbbbbb:\")\n",
    "    # print(cdf)\n",
    "    randnum = np.random.random()\n",
    "    for i in range(len(cdf) - 1):\n",
    "        if((randnum>cdf[i]) and (randnum<=cdf[i + 1])):\n",
    "            return i\n",
    "\n",
    "def IndexToState(index, xbins, ybins):\n",
    "    # print(\"aaaaaa:\")\n",
    "    # print(index)\n",
    "    x = np.floor(index/(ybins + 1))\n",
    "    y = index % (ybins + 1)\n",
    "    return x, y\n",
    "\n",
    "def StateToIndex(x, y, xbins, ybins):\n",
    "    i = xbins * np.floor(y) + np.floor(x) + 1\n",
    "    # print(\"aaaa\")\n",
    "    # print(x)\n",
    "    # print(y)\n",
    "    # print(i)\n",
    "    return int(i)\n",
    "    \n",
    "    return int((ybins + 1) * np.floor(x) + np.floor(y))\n",
    "        \n",
    "def NextStates(TM, particles, xbins, ybins):\n",
    "    cdf = np.apply_along_axis(CumulativeSum, 1, TM)\n",
    "    zero = np.array([0]) # np.zeros((len(cdf[:,1]))\n",
    "    \n",
    "    c = np.tile(zero[np.newaxis,:], (cdf.shape[0],1))\n",
    "    cdf = np.concatenate((c, cdf), axis=1)\n",
    "    \n",
    "    # cdf = np.concatenate([np.zeros((len(cdf[:,1])), 1), cdf])\n",
    "    # next_states = []\n",
    "    print(len(particles))\n",
    "    next_states = np.empty((len(particles), 2))\n",
    "    \n",
    "    for i in range(len(particles)):\n",
    "        current_state_x = particles[i, 0]\n",
    "        current_state_y = particles[i, 1]\n",
    "        \n",
    "        row = StateToIndex(current_state_x, current_state_y, xbins, ybins)\n",
    "        # print(cdf)\n",
    "        next_state_index = returnobj(cdf[row])\n",
    "        next_states[i, 0] = IndexToState(next_state_index, xbins, ybins)[0]\n",
    "        next_states[i, 1] = IndexToState(next_state_index, xbins, ybins)[1]\n",
    "    \n",
    "    # for (state, state_p) in zip(transitions,transitions[1:]):\n",
    "        # i = state[0]*xbins + state[1]\n",
    "        # j = state_p[0]*xbins + state_p[1]\n",
    "    \n",
    "    # print(len(next_states))\n",
    "    # print(next_states[:,0])\n",
    "    return next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "##### Predict Step #####\n",
    "########################\n",
    "\n",
    "def predict(particles, xbins, ybins):\n",
    "    next_states = NextStates(TM, particles, xbins, ybins)\n",
    "    particles[:, 0] += next_states[:, 0]\n",
    "    particles[:, 1] += next_states[:, 1]\n",
    "    \n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.monte_carlo import systematic_resample\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import randn\n",
    "import scipy.stats\n",
    "\n",
    "def DistanceToRSSI(agent_pos):\n",
    "    rssi_values = []\n",
    "    for b in beacons:\n",
    "        dist = int(round(np.linalg.norm(b - agent_pos)))\n",
    "        \n",
    "        # Max reading is 7 meters\n",
    "        if (dist > 7):\n",
    "            dist = 7\n",
    "        \n",
    "        print(dist)\n",
    "        mean = beacons_mean[dist - 1]\n",
    "        std = beacons_stds[dist - 1]\n",
    "        \n",
    "        sample = np.random.normal(mean, std, 1)\n",
    "        rssi_values.append(sample[0])\n",
    "        \n",
    "    return rssi_values\n",
    "\n",
    "def run_pf1(N, iters=18, sensor_std_err=.1, do_plot=True, plot_particles=False, xlim=(0, 20), ylim=(0, 20), initial_x=None):\n",
    "    plt.figure()\n",
    "   \n",
    "    # create particles and weights\n",
    "    particles = create_uniform_particles((0,20), (0,20), (0, 6.28), N)    \n",
    "    weights = np.ones(N) / N\n",
    "\n",
    "    if plot_particles:\n",
    "        alpha = .20\n",
    "        if N > 5000:\n",
    "            alpha *= np.sqrt(5000)/np.sqrt(N)           \n",
    "        plt.scatter(particles[:, 0], particles[:, 1], alpha=alpha, color='g')\n",
    "    \n",
    "    xs = []\n",
    "    robot_pos = np.array([0., 0.])\n",
    "    step = 1\n",
    "    for gt in range(len(oracle)):\n",
    "        print(step)\n",
    "        step = step + 1\n",
    "        # Agent's current location\n",
    "        agent_pos = oracle[gt]\n",
    "        print(\"aaaa\")\n",
    "        print(agent_pos)\n",
    "        # move particles based on transition matrix\n",
    "        # predict(particles, u=(0.00, 1.414), std=(.2, .05))\n",
    "        particles = predict(particles, xbins, ybins)\n",
    "        \n",
    "        # Observation from Beacons' RSSI\n",
    "        zs =  DistanceToRSSI(agent_pos)\n",
    "        \n",
    "        # Weighting particles based on our observation \n",
    "        update(particles, weights, z = zs, R = sensor_std_err)\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        # resample if too few effective particles\n",
    "        if neff(weights) < N/2:\n",
    "            indexes = systematic_resample(weights)\n",
    "            resample_from_index(particles, weights, indexes)\n",
    "            assert np.allclose(weights, 1/N)\n",
    "        mu, var = estimate(particles, weights)\n",
    "        xs.append(mu)\n",
    "\n",
    "        if plot_particles:\n",
    "            plt.scatter(particles[:, 0], particles[:, 1], color='k', marker=',', s=1)\n",
    "        \n",
    "        p1 = plt.scatter(robot_pos[0], robot_pos[1], marker='+', color='k', s=180, lw=3)\n",
    "        p2 = plt.scatter(mu[0], mu[1], marker='s', color='r')\n",
    "    \n",
    "    xs = np.array(xs)\n",
    "    #plt.plot(xs[:, 0], xs[:, 1])\n",
    "    plt.legend([p1, p2], ['Actual', 'PF'], loc=4, numpoints=1)\n",
    "    plt.xlim(*xlim)\n",
    "    plt.ylim(*ylim)\n",
    "    print('final position error, variance:\\n\\t', mu - np.array([iters, iters]), var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "aaaa\n",
      "[2.298139, 5.403299]\n",
      "5000\n",
      "5\n",
      "1\n",
      "2\n",
      "5\n",
      "5\n",
      "6\n",
      "3\n",
      "6\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "update() missing 1 required positional argument: 'beacons'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-ffb610d1d363>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrun_pf1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_particles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-130-7a14ca5aad37>\u001b[0m in \u001b[0;36mrun_pf1\u001b[1;34m(N, iters, sensor_std_err, do_plot, plot_particles, xlim, ylim, initial_x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# Weighting particles based on our observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparticles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msensor_std_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: update() missing 1 required positional argument: 'beacons'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(2) \n",
    "run_pf1(N=5000, plot_particles=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Our 2-dimensional distribution will be over variables X and Y\n",
    "N = 100\n",
    "X = np.linspace(49, 51, N)\n",
    "Y = np.linspace(49, 51, N)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "# Mean vector and covariance matrix\n",
    "mu = np.array([50., 50.])\n",
    "Sigma = np.array([[ 0.2 , 0.], [0.,  0.2]])\n",
    "\n",
    "# Pack X and Y into a single 3-dimensional array\n",
    "pos = np.empty(X.shape + (2,))\n",
    "pos[:, :, 0] = X\n",
    "pos[:, :, 1] = Y\n",
    "\n",
    "def multivariate_gaussian(pos, mu, Sigma):\n",
    "    \"\"\"Return the multivariate Gaussian distribution on array pos.\n",
    "\n",
    "    pos is an array constructed by packing the meshed arrays of variables\n",
    "    x_1, x_2, x_3, ..., x_k into its _last_ dimension.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n = mu.shape[0]\n",
    "    Sigma_det = np.linalg.det(Sigma)\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    N = np.sqrt((2*np.pi)**n * Sigma_det)\n",
    "    # This einsum call calculates (x-mu)T.Sigma-1.(x-mu) in a vectorized\n",
    "    # way across all the input variables.\n",
    "    fac = np.einsum('...k,kl,...l->...', pos-mu, Sigma_inv, pos-mu)\n",
    "\n",
    "    return np.exp(-fac / 2) / N\n",
    "\n",
    "# The distribution on the variables X, Y packed into pos.\n",
    "Z = multivariate_gaussian(pos, mu, Sigma)\n",
    "grid = Z\n",
    "# Create a surface plot and projected filled contour plot under it.\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, rstride=3, cstride=3, linewidth=1, antialiased=True, cmap=cm.viridis)\n",
    "\n",
    "# cset = ax.contourf(X, Y, Z, zdir='z', offset=-0.15, cmap=cm.viridis)\n",
    "\n",
    "# Adjust the limits, ticks and view angle\n",
    "# ax.set_zlim(-0.15,0.2)\n",
    "# ax.set_zticks(np.linspace(0,0.2,5))\n",
    "ax.view_init(30, 50)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "import pylab as pl\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.colors import PowerNorm\n",
    "\n",
    "# im = plt.matshow(zip(*error_heatmap_averaged), cmap=pl.cm.RdYlGn_r)\n",
    "im = plt.matshow(grid, cmap=pl.cm.OrRd)\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.colors import PowerNorm\n",
    "\n",
    "# im = plt.matshow(zip(*error_heatmap_averaged), cmap=pl.cm.RdYlGn_r)\n",
    "im = plt.matshow(grid, cmap=pl.cm.OrRd)\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.ndimage as sp\n",
    "\n",
    "x = np.random.randn(100000)\n",
    "y = np.random.randn(100000)\n",
    "\n",
    "\n",
    "X = sp.filters.gaussian_filter(x, sigma = 2, order = 0)\n",
    "Y = sp.filters.gaussian_filter(y, sigma = 2, order = 0)\n",
    "\n",
    "# normal distribution center at x=0 and y=5\n",
    "fig1 = plt.subplot(2,2,1)\n",
    "plt.hist2d(x, y, bins=40)\n",
    "plt.colorbar()\n",
    "plt.title('Heatmap without smoothing')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a colorbar if necessary\n",
    "plt.hexbin(x, y, gridsize=(25,25), cmap=plt.cm.Purples_r)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
